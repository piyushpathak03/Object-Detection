{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RCNN.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-Byq1w9gK29"
      },
      "source": [
        "!git clone https://github.com/1297rohit/RCNN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdUpZ8SWgSJo"
      },
      "source": [
        "!unzip /content/RCNN/Images.zip\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ooZ7mKOgaWh"
      },
      "source": [
        "!unzip /content/RCNN/Airplanes_Annotations.zip\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRmLpHJMgpby"
      },
      "source": [
        "import os,cv2,keras\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlULY9yRg3j-"
      },
      "source": [
        "### First step is to import all the libraries which will be needed to implement R-CNN. We need cv2 to perform selective search on the images. To use selective search we need to download opencv-contrib-python. To download that just run pip install opencv-contrib-python in the terminal and install it from pypi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3wLnOCog0Qd"
      },
      "source": [
        "path = \"Images\"\r\n",
        "annot = \"Airplanes_Annotations\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKOnVi6Hg6qr"
      },
      "source": [
        "for e,i in enumerate(os.listdir(annot)):\r\n",
        "    if e < 10:\r\n",
        "        filename = i.split(\".\")[0]+\".jpg\"\r\n",
        "        print(filename)\r\n",
        "        img = cv2.imread(os.path.join(path,filename))\r\n",
        "        df = pd.read_csv(os.path.join(annot,i))\r\n",
        "        plt.imshow(img)\r\n",
        "        for row in df.iterrows():\r\n",
        "            x1 = int(row[1][0].split(\" \")[0])\r\n",
        "            y1 = int(row[1][0].split(\" \")[1])\r\n",
        "            x2 = int(row[1][0].split(\" \")[2])\r\n",
        "            y2 = int(row[1][0].split(\" \")[3])\r\n",
        "            cv2.rectangle(img,(x1,y1),(x2,y2),(255,0,0), 2)\r\n",
        "        plt.figure()\r\n",
        "        plt.imshow(img)\r\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lefPb2wdg9g-"
      },
      "source": [
        "cv2.setUseOptimized(True);\r\n",
        "ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNHR1qS_hCej"
      },
      "source": [
        "im = cv2.imread(os.path.join(path,\"42849.jpg\"))\r\n",
        "ss.setBaseImage(im)\r\n",
        "ss.switchToSelectiveSearchFast()\r\n",
        "rects = ss.process()\r\n",
        "imOut = im.copy()\r\n",
        "for i, rect in (enumerate(rects)):\r\n",
        "    x, y, w, h = rect\r\n",
        "#     print(x,y,w,h)\r\n",
        "#     imOut = imOut[x:x+w,y:y+h]\r\n",
        "    cv2.rectangle(imOut, (x, y), (x+w, y+h), (0, 255, 0), 1, cv2.LINE_AA)\r\n",
        "# plt.figure()\r\n",
        "plt.imshow(imOut)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc0d-cAdhX8P"
      },
      "source": [
        "### After downloading opencv-contrib we need to initialise selective search. For that we have added the above step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTPqM1_ghQEl"
      },
      "source": [
        "def get_iou(bb1, bb2):\r\n",
        "    assert bb1['x1'] < bb1['x2']\r\n",
        "    assert bb1['y1'] < bb1['y2']\r\n",
        "    assert bb2['x1'] < bb2['x2']\r\n",
        "    assert bb2['y1'] < bb2['y2']\r\n",
        "\r\n",
        "    x_left = max(bb1['x1'], bb2['x1'])\r\n",
        "    y_top = max(bb1['y1'], bb2['y1'])\r\n",
        "    x_right = min(bb1['x2'], bb2['x2'])\r\n",
        "    y_bottom = min(bb1['y2'], bb2['y2'])\r\n",
        "\r\n",
        "    if x_right < x_left or y_bottom < y_top:\r\n",
        "        return 0.0\r\n",
        "\r\n",
        "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\r\n",
        "\r\n",
        "    bb1_area = (bb1['x2'] - bb1['x1']) * (bb1['y2'] - bb1['y1'])\r\n",
        "    bb2_area = (bb2['x2'] - bb2['x1']) * (bb2['y2'] - bb2['y1'])\r\n",
        "\r\n",
        "    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\r\n",
        "    assert iou >= 0.0\r\n",
        "    assert iou <= 1.0\r\n",
        "    return iou"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOErHc_Hhu-z"
      },
      "source": [
        "### Now we are initialising the function to calculate IOU (Intersection Over Union) of the ground truth box from the box computed by selective search. To understand more about calculating IOU you can refer to the link below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQx1-x-NhcVi"
      },
      "source": [
        "train_images=[]\r\n",
        "train_labels=[]\r\n",
        "for e,i in enumerate(os.listdir(annot)):\r\n",
        "    try:\r\n",
        "        if i.startswith(\"airplane\"):\r\n",
        "            filename = i.split(\".\")[0]+\".jpg\"\r\n",
        "            print(e,filename)\r\n",
        "            image = cv2.imread(os.path.join(path,filename))\r\n",
        "            df = pd.read_csv(os.path.join(annot,i))\r\n",
        "            gtvalues=[]\r\n",
        "            for row in df.iterrows():\r\n",
        "                x1 = int(row[1][0].split(\" \")[0])\r\n",
        "                y1 = int(row[1][0].split(\" \")[1])\r\n",
        "                x2 = int(row[1][0].split(\" \")[2])\r\n",
        "                y2 = int(row[1][0].split(\" \")[3])\r\n",
        "                gtvalues.append({\"x1\":x1,\"x2\":x2,\"y1\":y1,\"y2\":y2})\r\n",
        "            ss.setBaseImage(image)\r\n",
        "            ss.switchToSelectiveSearchFast()\r\n",
        "            ssresults = ss.process()\r\n",
        "            imout = image.copy()\r\n",
        "            counter = 0\r\n",
        "            falsecounter = 0\r\n",
        "            flag = 0\r\n",
        "            fflag = 0\r\n",
        "            bflag = 0\r\n",
        "            for e,result in enumerate(ssresults):\r\n",
        "                if e < 2000 and flag == 0:\r\n",
        "                    for gtval in gtvalues:\r\n",
        "                        x,y,w,h = result\r\n",
        "                        iou = get_iou(gtval,{\"x1\":x,\"x2\":x+w,\"y1\":y,\"y2\":y+h})\r\n",
        "                        if counter < 30:\r\n",
        "                            if iou > 0.70:\r\n",
        "                                timage = imout[y:y+h,x:x+w]\r\n",
        "                                resized = cv2.resize(timage, (224,224), interpolation = cv2.INTER_AREA)\r\n",
        "                                train_images.append(resized)\r\n",
        "                                train_labels.append(1)\r\n",
        "                                counter += 1\r\n",
        "                        else :\r\n",
        "                            fflag =1\r\n",
        "                        if falsecounter <30:\r\n",
        "                            if iou < 0.3:\r\n",
        "                                timage = imout[y:y+h,x:x+w]\r\n",
        "                                resized = cv2.resize(timage, (224,224), interpolation = cv2.INTER_AREA)\r\n",
        "                                train_images.append(resized)\r\n",
        "                                train_labels.append(0)\r\n",
        "                                falsecounter += 1\r\n",
        "                        else :\r\n",
        "                            bflag = 1\r\n",
        "                    if fflag == 1 and bflag == 1:\r\n",
        "                        print(\"inside\")\r\n",
        "                        flag = 1\r\n",
        "    except Exception as e:\r\n",
        "        print(e)\r\n",
        "        print(\"error in \"+filename)\r\n",
        "        continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeOXXKWFiWWF"
      },
      "source": [
        "### The above code is pre-processing and creating the data-set to pass to the model. As in this case we can have 2 classes. These classes are that whether the proposed region can be a foreground (i.e. Airplane) or a background. So we will set the label of foreground (i.e. Airplane) as 1 and the label of background as 0. The following steps are being performed in the above code block.\r\n",
        "1. Loop over the image folder and set each image one by one as the base for selective search using code ss.setBaseImage(image)\r\n",
        "\r\n",
        "2. Initialising fast selective search and getting proposed regions using using code ss.switchToSelectiveSearchFast() and ssresults = ss.process()\r\n",
        "\r\n",
        "3. Iterating over all the first 2000 results passed by selective search and calculating IOU of the proposed region and annotated region using the get_iou() function created above.\r\n",
        "\r\n",
        "4. Now as one image can many negative sample (i.e. background) and just some positive sample (i.e. airplane) so we need to make sure that we have good proportion of both positive and negative sample to train our model. Therefore we have set that we will collect maximum of 30 negative sample (i.e. background) and positive sample (i.e. airplane) from one image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsEvUgvsh3v2"
      },
      "source": [
        "### After running the above code snippet our training data will be ready. List train_images=[] will contain all the images and train_labels=[] will contain all the labels marking airplane images as 1 and non airplane images (i.e. background images) as 0.\r\n",
        "\r\n",
        "X_new = np.array(train_images)\r\n",
        "y_new = np.array(train_labels)\r\n",
        "\r\n",
        "X_new.shape\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoT3lXVLitqq"
      },
      "source": [
        "### After completing the process of creating the dataset we will convert the array to numpy array so that we can traverse it easily and pass the datatset to the model in an efficient way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EySCtP9qiqnw"
      },
      "source": [
        "from keras.layers import Dense\r\n",
        "from keras import Model\r\n",
        "from keras import optimizers\r\n",
        "from keras.preprocessing.image import ImageDataGenerator\r\n",
        "from keras.applications.vgg16 import VGG16\r\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-z3LJS3i6Wv"
      },
      "source": [
        "# add preprocessing layer to the front of VGG\r\n",
        "\r\n",
        "vggmodel = VGG16(weights='imagenet', include_top=True)\r\n",
        "\r\n",
        "\r\n",
        "# don't train existing weights\r\n",
        "for layers in (vggmodel.layers)[:15]:\r\n",
        "    print(layers)\r\n",
        "    layers.trainable = False\r\n",
        "  \r\n",
        "\r\n",
        "# our layers - you can add more if you want\r\n",
        "X= vggmodel.layers[-2].output\r\n",
        "\r\n",
        "predictions = Dense(2, activation=\"softmax\")(X)\r\n",
        "\r\n",
        "# create a model object\r\n",
        "model_final = Model(inputs = vggmodel.input, outputs = predictions)\r\n",
        "\r\n",
        "opt = Adam(lr=0.0001)\r\n",
        "\r\n",
        "model_final.compile(loss = keras.losses.categorical_crossentropy, optimizer = opt, metrics=[\"accuracy\"])\r\n",
        "\r\n",
        "# In this part in the loop we are freezing the first 15 layers of the model. After that we are taking out the second last layer of the model and then adding a 2 unit softmax dense layer as we have just 2 classes to predict i.e. foreground or background. After that we are compiling the model using Adam optimizer with learning rate of 0.001. We are using categorical_crossentropy as loss since the output of the model is categorical. Finally the summary of the model will is printed using model_final.summary(). The image of summary is attached below.\r\n",
        "\r\n",
        "# view the structure of the model\r\n",
        "model_final.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vF14jEF-kdn2"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.preprocessing import LabelBinarizer\r\n",
        "class MyLabelBinarizer(LabelBinarizer):\r\n",
        "    def transform(self, y):\r\n",
        "        Y = super().transform(y)\r\n",
        "        if self.y_type_ == 'binary':\r\n",
        "            return np.hstack((Y, 1-Y))\r\n",
        "        else:\r\n",
        "            return Y\r\n",
        "    def inverse_transform(self, Y, threshold=None):\r\n",
        "        if self.y_type_ == 'binary':\r\n",
        "            return super().inverse_transform(Y[:, 0], threshold)\r\n",
        "        else:\r\n",
        "            return super().inverse_transform(Y, threshold)\r\n",
        "lenc = MyLabelBinarizer()\r\n",
        "Y =  lenc.fit_transform(y_new)\r\n",
        "\r\n",
        "\r\n",
        "X_train, X_test , y_train, y_test = train_test_split(X_new,Y,test_size=0.10)\r\n",
        "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hINn3jxkfsd"
      },
      "source": [
        "### After creating the model now we need to split the dataset into train and test set. Before that we need to one-hot encode the label. For that we are using MyLabelBinarizer() and encoding the dataset. Then we are splitting the dataset using train_test_split from sklearn. We are keeping 10% of the dataset as test set and 90% as training set.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsH-NKaqkk2c"
      },
      "source": [
        "trdata = ImageDataGenerator(horizontal_flip=True, vertical_flip=True, rotation_range=90)\r\n",
        "traindata = trdata.flow(x=X_train, y=y_train)\r\n",
        "tsdata = ImageDataGenerator(horizontal_flip=True, vertical_flip=True, rotation_range=90)\r\n",
        "testdata = tsdata.flow(x=X_test, y=y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCkQVMsFkviJ"
      },
      "source": [
        "### Now we will use Keras ImageDataGenerator to pass the dataset to the model. We will do some augmentation on the dataset like horizontal flip, vertical flip and rotation to increase the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI_-wJpbkrB4"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\r\n",
        "checkpoint = ModelCheckpoint(\"ieeercnn_vgg16_1.h5\", monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\r\n",
        "early = EarlyStopping(monitor='val_loss', min_delta=0, patience=100, verbose=1, mode='auto')\r\n",
        "hist = model_final.fit_generator(generator= traindata, steps_per_epoch= 10, epochs= 1000, validation_data= testdata, validation_steps=2, callbacks=[checkpoint,early])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmdFRTkWlBjE"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "# plt.plot(hist.history[\"acc\"])\r\n",
        "# plt.plot(hist.history['val_acc'])\r\n",
        "plt.plot(hist.history['loss'])\r\n",
        "plt.plot(hist.history['val_loss'])\r\n",
        "plt.title(\"model loss\")\r\n",
        "plt.ylabel(\"Loss\")\r\n",
        "plt.xlabel(\"Epoch\")\r\n",
        "plt.legend([\"Loss\",\"Validation Loss\"])\r\n",
        "plt.show()\r\n",
        "plt.savefig('chart loss.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JktEF3LRlCj2"
      },
      "source": [
        "im = X_test[1600]\r\n",
        "plt.imshow(im)\r\n",
        "img = np.expand_dims(im, axis=0)\r\n",
        "out= model_final.predict(img)\r\n",
        "if out[0][0] > out[0][1]:\r\n",
        "    print(\"plane\")\r\n",
        "else:\r\n",
        "    print(\"not plane\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjAS8mJtk4au"
      },
      "source": [
        "### Now we start the training of the model using fit_generator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHE4EmVYk0ld"
      },
      "source": [
        "z=0\r\n",
        "for e,i in enumerate(os.listdir(path)):\r\n",
        "    if i.startswith(\"4\"):\r\n",
        "        z += 1\r\n",
        "        img = cv2.imread(os.path.join(path,i))\r\n",
        "        ss.setBaseImage(img)\r\n",
        "        ss.switchToSelectiveSearchFast()\r\n",
        "        ssresults = ss.process()\r\n",
        "        imout = img.copy()\r\n",
        "        for e,result in enumerate(ssresults):\r\n",
        "            if e < 2000:\r\n",
        "                x,y,w,h = result\r\n",
        "                timage = imout[y:y+h,x:x+w]\r\n",
        "                resized = cv2.resize(timage, (224,224), interpolation = cv2.INTER_AREA)\r\n",
        "                img = np.expand_dims(resized, axis=0)\r\n",
        "                out= model_final.predict(img)\r\n",
        "                if out[0][0] > 0.70:\r\n",
        "                    cv2.rectangle(imout, (x, y), (x+w, y+h), (0, 255, 0), 1, cv2.LINE_AA)\r\n",
        "        plt.figure()\r\n",
        "        plt.imshow(imout)\r\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgNZL_MLlJXZ"
      },
      "source": [
        "### Now once we have created the model. We need to do prediction on that model. For that we need to follow the steps mentioned below :\r\n",
        "\r\n",
        "1. pass the image from selective search.\r\n",
        "\r\n",
        "2. pass all the result of the selective search to the model as input using model_final.predict(img).\r\n",
        "\r\n",
        "3. If the output of the model says the region to be a foreground image (i.e. airplane image) and if the confidence is above the defined threshold then create bounding box on the original image on the coordinate of the proposed region."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "787UKwwLlci6"
      },
      "source": [
        "### ### As you can see above we created box on the proposed region in which the accuracy of the model was above 0.70. In this way we can do localisation on an image and perform object detection using R-CNN. This is how we implement an R-CNN architecture from scratch using keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxQV4e-QlWaR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}